import numpy as np
import matplotlib.pyplot as plt
import time


start=time.time()

img= plt.imread('The Image')
(h,w,d)= img.shape
X0= np.array(np.reshape(img, (h*w,d)))

# KMeans first
num_clust= 4   

emeans= X0[np.random.choice(X0.shape[0], num_clust, replace=False ), :]

print("Initialised Centroids--", '\n', emeans)

print()

N= len(X0)

labels= np.zeros(shape=(N,))

def min_dist(point, p_mat):
    return np.argmin(np.apply_along_axis(np.linalg.norm, 1, point-p_mat))

def assignmt(X, centroids):
    n= len(centroids)
    for j in range(N):
        labels[j]= min_dist(X[j], centroids)
        
    for i in range(n):
        if [labels==i]:
            centroids[i]= np.mean(X[labels==i], axis=0)
    return centroids

#Iterations 

num_iter= 50

for k in range(num_iter):
    emeans= assignmt(X0, emeans)

ecovs= np.array([np.cov((X0[labels==i]).T) for i in range(num_clust)])
ewts= np.array([(X0[labels==i]).shape[0] for i in range(num_clust)])/N


print('New centroids--', '\n', emeans , '\n')
print()
print('Intra-cluster Covariance Matrices--', '\n', ecovs, '\n')
print()
print('New weights--', '\n', ewts)
print()

#Proceeding to use these results as initialisations for the GMM

likelihoods=[]

def pdf_norm(x, mean, cov):
    return ((2*np.pi)**(-len(x)/2)) * ((np.linalg.det(cov))**(-0.5) ) * np.exp(-0.5*((x-mean).T@np.linalg.inv(cov)@(x-mean)))


def E_Step(X, means, covs, weights):
    
    def mix_pdf(x):
        M= len(means)
        return np.array([pdf_norm(x, means[k], covs[k]) for k in range(M)])
    
    mixt = np.apply_along_axis(mix_pdf, 1, X) 
    
    wts_rep = np.tile(weights, (N, 1)) 

    U= mixt*wts_rep
    V= U/U.sum(axis=1)[:, None]
    print(np.sum(U, axis=1))
    
    l_hood= np.sum(np.log(np.sum(U, axis=1)))
    print("Likelihood:", l_hood)
    likelihoods.append(l_hood)
    
    return V

def M_Step(X, R, means, covs, weights):
    M= len(means)
    R_sum= np.sum(R, axis=0)
    
    #Updating the weights
    new_wts= R_sum/N 
    
    #Updating the covariance matrices
    for n in range(M):
        new_mat= np.einsum('ij, j ->ij', (X-means[n]).T, (R.T)[n])
        covs[n]= new_mat@(X-means[n])
    
    new_covs= np.einsum('ijk,i->ijk', covs, 1/R_sum)
    
    #Updating the means
    
    new_means= ((X.T @R)/R_sum).T
    
    return new_means, new_covs, new_wts

#EM Iterations
diff, count= 1,1

resp= E_Step(X0, emeans, ecovs, ewts)
emeans, ecovs, ewts = M_Step(X0, resp, emeans, ecovs, ewts)

while diff > 0.1 :
    resp= E_Step(X0, emeans, ecovs, ewts)
    emeans, ecovs, ewts = M_Step(X0, resp, emeans, ecovs, ewts)
    diff= likelihoods[-1]- likelihoods[-2]
    count +=1
    

resp_mat= E_Step(X0, emeans, ecovs, ewts)
Y= np.array([emeans[np.argmax(resp_mat[i])].astype(int) for i in range(N)])

new_img= Y.reshape(h,w,d)
x_vals=[i for i in range(count+1)]

plt.figure()

plt.imshow(new_img)

plt.figure()
plt.plot(x_vals, likelihoods)
plt.xlabel('Number of Iterations')
plt.ylabel('Expected Log Likelihood')

plt.show()

end=time.time()

print('Final Means:', '\n', emeans, '\n')
print('Final Covariance Matrices:', '\n', ecovs, '\n')
print('Final Weights:', '\n', ewts, '\n')

print('Number of Iterations till convergence: ', '\n', count, '\n')

print('Runtime(s)', '\n', end-start)








    

